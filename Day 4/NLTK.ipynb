{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f3e2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef55902",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = 'It was a very pleasant day. The weather was cool and there were light showers. I went to the market to buy some fruits.'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "620a23cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a very pleasant day. The weather was cool and there were light showers. I went to the market to buy some fruits.\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc41bd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35901f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93178da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " 'was',\n",
       " 'a',\n",
       " 'very',\n",
       " 'pleasant',\n",
       " 'day',\n",
       " '.',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'was',\n",
       " 'cool',\n",
       " 'and',\n",
       " 'there',\n",
       " 'were',\n",
       " 'light',\n",
       " 'showers',\n",
       " '.',\n",
       " 'I',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'market',\n",
       " 'to',\n",
       " 'buy',\n",
       " 'some',\n",
       " 'fruits',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35faf9f0",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a36d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "581fa50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac8f6068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mohit/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4f31c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mohit/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "243ce469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/mohit/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52335df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc5ae8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcb860b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "340aaaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    useful_words = []\n",
    "    \n",
    "    for word in text:\n",
    "        if word.lower() not in sw:\n",
    "            useful_words.append(word)\n",
    "            \n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2fae709a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pleasant',\n",
       " 'day',\n",
       " '.',\n",
       " 'weather',\n",
       " 'cool',\n",
       " 'light',\n",
       " 'showers',\n",
       " '.',\n",
       " 'went',\n",
       " 'market',\n",
       " 'buy',\n",
       " 'fruits',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d633b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', '50', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'mohit', '@', 'codingminutes.com']\n",
      "['Send', '50', 'documents', 'related', 'chapters', '1,2,3', 'mohit', '@', 'codingminutes.com']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at mohit@codingminutes.com\"\n",
    "\n",
    "sentence = word_tokenize(sentence)\n",
    "print(sentence)\n",
    "sentence = remove_stopwords(sentence)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2be08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb606d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e08f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c308bd93",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60f6b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \"\"\"Foxes love to make jumps. The quick brown fox was seen jumping over the lovely dog from a 6ft feet high wall\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a184dcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foxes love to make jumps.The quick brown fox was seen jumping over the lovely dog from a 6ft feet high wall\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dbe27e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ab25ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss  = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3707f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('loved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af16be6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e3d663e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "277be14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8f7f0319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    "    words = word_tokenize(data)\n",
    "    \n",
    "    useful_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        useful_words.append( ss.stem(word) )\n",
    "    \n",
    "    return useful_words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee398592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fox',\n",
       " 'love',\n",
       " 'to',\n",
       " 'make',\n",
       " 'jump',\n",
       " '.',\n",
       " 'the',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'fox',\n",
       " 'was',\n",
       " 'seen',\n",
       " 'jump',\n",
       " 'over',\n",
       " 'the',\n",
       " 'love',\n",
       " 'dog',\n",
       " 'from',\n",
       " 'a',\n",
       " '6ft',\n",
       " 'feet',\n",
       " 'high',\n",
       " 'wall']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa4679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea6f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f68ec3b2",
   "metadata": {},
   "source": [
    "# Data Cleaning/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9fd9c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people.',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2a335b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(document):\n",
    "    document = document.lower()\n",
    "    words = word_tokenize(document)\n",
    "    \n",
    "    # list comprehension\n",
    "    # stopword removal + stemming\n",
    "    words = [ss.stem(w) for w in words if w not in sw and len(w)>1]\n",
    "    \n",
    "    # convert tokens into string\n",
    "    cleaned = \" \".join(words)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e5e5eaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'indian love cricket team win world cup say capt virat koh world cup held sri lanka'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning('Indian loving cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "aa00ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dataset = []\n",
    "\n",
    "for document in corpus:\n",
    "    c = data_cleaning(document)\n",
    "    cleaned_dataset.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8811ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indian cricket team win world cup say capt virat koh world cup held sri lanka',\n",
       " 'win next lok sabha elect say confid indian pm',\n",
       " 'nobel laurat heart peopl',\n",
       " 'movi raazi excit indian spi thriller base upon real stori']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d04aa97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9e147e5",
   "metadata": {},
   "source": [
    "# Building a Vocab & Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7eb98c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of word\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3257bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea1bbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb28a5bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f1fb33b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 42)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ead638b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 2],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "695d5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e1e87430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "24cd6f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(vectorized_corpus, columns= feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "af8fb43e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>at</th>\n",
       "      <th>based</th>\n",
       "      <th>be</th>\n",
       "      <th>capt</th>\n",
       "      <th>confident</th>\n",
       "      <th>cricket</th>\n",
       "      <th>cup</th>\n",
       "      <th>elections</th>\n",
       "      <th>exciting</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>thriller</th>\n",
       "      <th>upon</th>\n",
       "      <th>virat</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>win</th>\n",
       "      <th>wins</th>\n",
       "      <th>won</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  at  based  be  capt  confident  cricket  cup  elections  exciting  ...  \\\n",
       "0   0   1      0   1     1          0        1    2          0         0  ...   \n",
       "1   0   0      0   0     0          1        0    0          1         0  ...   \n",
       "2   0   0      0   0     0          0        0    0          0         0  ...   \n",
       "3   1   0      1   0     0          0        0    0          0         1  ...   \n",
       "\n",
       "   the  thriller  upon  virat  we  will  win  wins  won  world  \n",
       "0    0         0     0      1   0     2    0     1    0      2  \n",
       "1    0         0     0      0   1     1    1     0    0      0  \n",
       "2    3         0     0      0   0     0    0     0    1      0  \n",
       "3    1         1     1      0   0     0    0     0    0      0  \n",
       "\n",
       "[4 rows x 42 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "02a0825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"mohit confident virat an at world icecream mobile\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "30de5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectorized = cv.transform([test]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b63faa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d856f509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 42)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcea219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
